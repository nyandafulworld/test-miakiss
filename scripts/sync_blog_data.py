#!/usr/bin/env python3
"""
ãƒ–ãƒ­ã‚°è¨˜äº‹ãƒ‡ãƒ¼ã‚¿åŒæœŸã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’è‡ªå‹•ã§è¡Œã„ã¾ã™ï¼š
1. blog/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
2. HTMLã®ãƒ¡ã‚¿ã‚¿ã‚°ã‹ã‚‰è¨˜äº‹æƒ…å ±ã‚’æŠ½å‡º
3. published_articles.json ã«å­˜åœ¨ã—ãªã„æ–°è¨˜äº‹ã‚’è‡ªå‹•è¿½åŠ 
4. used_keywords.json ã‚‚åŒæ™‚ã«æ›´æ–°
5. published_articles.js ã‚’ç”Ÿæˆ

ä½¿ã„æ–¹:
    python3 scripts/sync_blog_data.py

ã“ã‚Œã«ã‚ˆã‚Šã€è¨˜äº‹HTMLã‚’ä½œæˆã—ãŸå¾Œã«ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã ã‘ã§ã€
å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè‡ªå‹•çš„ã«æ›´æ–°ã•ã‚Œã¾ã™ã€‚
"""

import json
import os
import re
from pathlib import Path
from datetime import datetime
import html.parser


class MetaTagParser(html.parser.HTMLParser):
    """HTMLã‹ã‚‰ãƒ¡ã‚¿ã‚¿ã‚°ã‚’è§£æã™ã‚‹ãƒ‘ãƒ¼ã‚µãƒ¼"""
    
    def __init__(self):
        super().__init__()
        self.title = None
        self.description = None
        self.date_published = None
        self.in_title = False
        self.title_content = []
        self.in_script = False
        self.script_content = []
        
    def handle_starttag(self, tag, attrs):
        attrs_dict = dict(attrs)
        
        if tag == 'title':
            self.in_title = True
            
        elif tag == 'meta':
            name = attrs_dict.get('name', '')
            content = attrs_dict.get('content', '')
            
            if name == 'description':
                self.description = content
                
        elif tag == 'script':
            script_type = attrs_dict.get('type', '')
            if script_type == 'application/ld+json':
                self.in_script = True
    
    def handle_endtag(self, tag):
        if tag == 'title':
            self.in_title = False
            self.title = ''.join(self.title_content).strip()
            # " | æ ªå¼ä¼šç¤¾ãƒŸã‚¢ã‚­ã‚¹" ã‚’é™¤å»
            if ' | æ ªå¼ä¼šç¤¾ãƒŸã‚¢ã‚­ã‚¹' in self.title:
                self.title = self.title.replace(' | æ ªå¼ä¼šç¤¾ãƒŸã‚¢ã‚­ã‚¹', '')
                
        elif tag == 'script' and self.in_script:
            self.in_script = False
            try:
                script_text = ''.join(self.script_content)
                json_data = json.loads(script_text)
                if json_data.get('@type') == 'Article':
                    self.date_published = json_data.get('datePublished')
            except (json.JSONDecodeError, TypeError):
                pass
            self.script_content = []
    
    def handle_data(self, data):
        if self.in_title:
            self.title_content.append(data)
        elif self.in_script:
            self.script_content.append(data)


def extract_article_info_from_html(html_path):
    """
    HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¡ã‚¿ã‚¿ã‚°ã‚’è§£æã—ã¦è¨˜äº‹æƒ…å ±ã‚’æŠ½å‡º
    
    Returns:
        dict: {title, description, date, slug} ã¾ãŸã¯ Noneï¼ˆè§£æå¤±æ•—æ™‚ï¼‰
    """
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        parser = MetaTagParser()
        parser.feed(content)
        
        # slugã¯ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å–å¾—ï¼ˆ.htmlã‚’é™¤å»ï¼‰
        slug = html_path.stem
        
        if parser.title and parser.date_published:
            return {
                'title': parser.title,
                'description': parser.description or '',
                'date': parser.date_published,
                'slug': slug
            }
        else:
            return None
            
    except Exception as e:
        print(f"âš ï¸ HTMLè§£æã‚¨ãƒ©ãƒ¼: {html_path} - {e}")
        return None


def find_keyword_info(slug, title, keywords_data):
    """
    ã‚¹ãƒ©ã‚°ã¨ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æƒ…å ±ã‚’æ¤œç´¢
    
    Returns:
        dict: {keyword, keywordId, category} ã¾ãŸã¯ None
    """
    # ã¾ãšpublished_articles.jsonã«æ—¢å­˜ã®ã‚¨ãƒ³ãƒˆãƒªãŒã‚ã‚‹ã‹ç¢ºèª
    # ãªã‘ã‚Œã°keywords.jsonã‹ã‚‰æ¤œç´¢
    
    keywords = keywords_data.get('keywords', [])
    
    # å®Œå…¨ä¸€è‡´æ¤œç´¢
    for kw in keywords:
        theme = kw.get('theme', '')
        # ãƒ†ãƒ¼ãƒãŒã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        if theme and theme in title:
            return {
                'keyword': kw.get('keyword'),
                'keywordId': kw.get('id'),
                'category': kw.get('category')
            }
    
    # éƒ¨åˆ†ä¸€è‡´æ¤œç´¢ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ï¼‰
    for kw in keywords:
        keyword = kw.get('keyword', '')
        if keyword and keyword in title:
            return {
                'keyword': keyword,
                'keywordId': kw.get('id'),
                'category': kw.get('category')
            }
    
    return None


def find_new_articles(blog_dir, published_articles, keywords_data):
    """
    published_articles.jsonã«å­˜åœ¨ã—ãªã„HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º
    
    Returns:
        list: æ–°ã—ã„è¨˜äº‹æƒ…å ±ã®ãƒªã‚¹ãƒˆ
    """
    existing_slugs = set()
    for article in published_articles.get('articles', []):
        existing_slugs.add(article.get('slug'))
    
    new_articles = []
    
    # blog/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
    for html_file in blog_dir.glob('*.html'):
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¯é™¤å¤–
        if 'template' in html_file.name.lower():
            continue
            
        slug = html_file.stem
        
        # æ—¢å­˜ã®è¨˜äº‹ã¯ã‚¹ã‚­ãƒƒãƒ—
        if slug in existing_slugs:
            continue
        
        # HTMLã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
        article_info = extract_article_info_from_html(html_file)
        if article_info:
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æƒ…å ±ã‚’æ¤œç´¢
            kw_info = find_keyword_info(slug, article_info['title'], keywords_data)
            if kw_info:
                article_info['keyword'] = kw_info['keyword']
                article_info['keywordId'] = kw_info['keywordId']
                article_info['category'] = kw_info['category']
            else:
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                article_info['keyword'] = ''
                article_info['category'] = 'other'
            
            new_articles.append(article_info)
            print(f"âœ¨ æ–°ã—ã„è¨˜äº‹ã‚’æ¤œå‡º: {slug}")
    
    return new_articles


def update_published_articles(published_articles, new_articles):
    """
    published_articles.json ã«æ–°ã—ã„è¨˜äº‹ã‚’è¿½åŠ 
    """
    for article in new_articles:
        entry = {
            'slug': article['slug'],
            'title': article['title'],
            'date': article['date'],
            'category': article.get('category', 'other'),
            'keyword': article.get('keyword', ''),
            'description': article['description']
        }
        
        # keywordIdãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
        if article.get('keywordId'):
            entry['keywordId'] = article['keywordId']
        
        published_articles['articles'].append(entry)
    
    # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
    published_articles['articles'].sort(key=lambda x: x.get('date', ''), reverse=True)
    
    # æ›´æ–°æ—¥æ™‚ã‚’è¨­å®š
    published_articles['lastUpdated'] = datetime.now().strftime('%Y-%m-%dT%H:%M:%S+09:00')
    
    return published_articles


def update_used_keywords(used_keywords, new_articles):
    """
    used_keywords.json ã‚’æ›´æ–°
    """
    for article in new_articles:
        keyword_id = article.get('keywordId')
        keyword = article.get('keyword', '')
        
        if not keyword:
            continue
        
        # used_keyword_ids ã«è¿½åŠ 
        if keyword_id and keyword_id not in used_keywords.get('used_keyword_ids', []):
            used_keywords.setdefault('used_keyword_ids', []).append(keyword_id)
        
        # used_keyword_texts ã«è¿½åŠ 
        if keyword and keyword not in used_keywords.get('used_keyword_texts', []):
            used_keywords.setdefault('used_keyword_texts', []).append(keyword)
        
        # keyword_usage_history ã«è¿½åŠ 
        history_entry = {
            'keywordId': keyword_id,
            'keyword': keyword,
            'date': article['date'],
            'slug': article['slug']
        }
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        existing_slugs = [h.get('slug') for h in used_keywords.get('keyword_usage_history', [])]
        if article['slug'] not in existing_slugs:
            used_keywords.setdefault('keyword_usage_history', []).append(history_entry)
    
    # æ›´æ–°æ—¥æ™‚ã‚’è¨­å®š
    used_keywords['last_updated'] = datetime.now().strftime('%Y-%m-%dT%H:%M:%S+09:00')
    
    return used_keywords


def generate_js_file(published_articles, js_file):
    """
    published_articles.js ã‚’ç”Ÿæˆ
    """
    js_content = f"""// ãƒ–ãƒ­ã‚°è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã§ã‚‚å‹•ä½œã™ã‚‹ã‚ˆã†ã«ã€JavaScriptãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦æä¾›ï¼‰
// ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ published_articles.json ã¨åŒã˜å†…å®¹ã‚’ä¿æŒã—ã¾ã™
// âš ï¸ ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã™ã€‚æ‰‹å‹•ã§ç·¨é›†ã—ãªã„ã§ãã ã•ã„ã€‚
// æ›´æ–°ã™ã‚‹å ´åˆã¯ published_articles.json ã‚’ç·¨é›†ã—ã€scripts/sync_blog_data.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚
window.BLOG_ARTICLES_DATA = {json.dumps(published_articles, ensure_ascii=False, indent=2)};
"""
    
    with open(js_file, 'w', encoding='utf-8') as f:
        f.write(js_content)
    
    return True


def sync_blog_data():
    """
    ãƒ¡ã‚¤ãƒ³ã®åŒæœŸå‡¦ç†
    """
    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’å–å¾—
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    blog_dir = project_root / 'blog'
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    published_json = blog_dir / 'published_articles.json'
    published_js = blog_dir / 'published_articles.js'
    used_keywords_json = blog_dir / 'used_keywords.json'
    keywords_json = blog_dir / 'keywords.json'
    
    # published_articles.json ã‚’èª­ã¿è¾¼ã‚€
    try:
        with open(published_json, 'r', encoding='utf-8') as f:
            published_articles = json.load(f)
    except FileNotFoundError:
        print(f"âš ï¸ {published_json} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
        published_articles = {'articles': [], 'lastUpdated': ''}
    except json.JSONDecodeError as e:
        print(f"âŒ JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return False
    
    # used_keywords.json ã‚’èª­ã¿è¾¼ã‚€
    try:
        with open(used_keywords_json, 'r', encoding='utf-8') as f:
            used_keywords = json.load(f)
    except FileNotFoundError:
        print(f"âš ï¸ {used_keywords_json} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
        used_keywords = {
            'used_keyword_ids': [],
            'used_keyword_texts': [],
            'keyword_usage_history': []
        }
    except json.JSONDecodeError as e:
        print(f"âŒ used_keywords.json JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return False
    
    # keywords.json ã‚’èª­ã¿è¾¼ã‚€ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æƒ…å ±æ¤œç´¢ç”¨ï¼‰
    try:
        with open(keywords_json, 'r', encoding='utf-8') as f:
            keywords_data = json.load(f)
    except FileNotFoundError:
        print(f"âš ï¸ {keywords_json} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        keywords_data = {'keywords': []}
    except json.JSONDecodeError as e:
        print(f"âš ï¸ keywords.json JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        keywords_data = {'keywords': []}
    
    # æ–°ã—ã„è¨˜äº‹ã‚’æ¤œå‡º
    new_articles = find_new_articles(blog_dir, published_articles, keywords_data)
    
    if new_articles:
        print(f"\nğŸ“ {len(new_articles)}ä»¶ã®æ–°ã—ã„è¨˜äº‹ã‚’è¿½åŠ ã—ã¾ã™")
        
        # published_articles.json ã‚’æ›´æ–°
        published_articles = update_published_articles(published_articles, new_articles)
        
        with open(published_json, 'w', encoding='utf-8') as f:
            json.dump(published_articles, f, ensure_ascii=False, indent=2)
        print(f"âœ… {published_json} ã‚’æ›´æ–°ã—ã¾ã—ãŸ")
        
        # used_keywords.json ã‚’æ›´æ–°
        used_keywords = update_used_keywords(used_keywords, new_articles)
        
        with open(used_keywords_json, 'w', encoding='utf-8') as f:
            json.dump(used_keywords, f, ensure_ascii=False, indent=2)
        print(f"âœ… {used_keywords_json} ã‚’æ›´æ–°ã—ã¾ã—ãŸ")
    else:
        print("ğŸ“‹ æ–°ã—ã„è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“")
    
    # published_articles.js ã‚’ç”Ÿæˆ
    try:
        generate_js_file(published_articles, published_js)
        print(f"âœ… {published_js} ã‚’æ›´æ–°ã—ã¾ã—ãŸ")
        print(f"ğŸ“Š è¨˜äº‹æ•°: {len(published_articles.get('articles', []))}ä»¶")
        return True
    except Exception as e:
        print(f"âŒ JSãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return False


if __name__ == '__main__':
    print("=" * 50)
    print("ãƒ–ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿åŒæœŸã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 50)
    print("\nğŸ”„ ãƒ–ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’åŒæœŸã—ã¦ã„ã¾ã™...\n")
    
    success = sync_blog_data()
    
    print()
    if success:
        print("=" * 50)
        print("âœ… åŒæœŸå®Œäº†")
        print("=" * 50)
    else:
        print("=" * 50)
        print("âŒ åŒæœŸå¤±æ•—")
        print("=" * 50)
        exit(1)
